{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 1 notMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://commodatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "\n",
    "def download_progress(count, blockSize, totalSize):\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "    \n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "    last_percent_reported = percent\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    if force or os.path.exists(filename):\n",
    "        print('Attempting to download: ', filename)\n",
    "        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print('download file size : %s' , statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "#train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "#test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "train_filename = 'notMNIST_large.tar.gz'\n",
    "test_filename = 'notMNIST_small.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large already present - Skpping extraction of notMNIST_large.tar.gz.\n",
      "['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']\n",
      "notMNIST_small already present - Skpping extraction of notMNIST_small.tar.gz.\n",
      "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(11)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - Skpping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))\n",
    "    ]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1\n",
    "### Let's take a peek at some of the data to make sure it looks sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/A\n",
      "Q2hlbHRlbmhhbUJULUl0YWxpYy5vdGY=.png\n",
      "notMNIST_small/A/Q2hlbHRlbmhhbUJULUl0YWxpYy5vdGY=.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABnklEQVR4nHWQTYhNYRzGf+/HDE2o\nmYbI1DRpdsgIkZEau5uSksTKRrKxmPK1lcxuJjZTSrZKKGTHLZMaZeMWKx9pSs2ElGvGvef8H4tz\nz3WOe/23z/t8/F4oXeCGThHodp5dy3qI/4/4QFoYwHXRAgdlqSby3FKCcQUzDnVzBo5o7p1U7SI6\n3Lx23pL9HOqcFDihR5zRbx3vgHH0vdE+xpJENzvEwDndwfu3Uu3fTse6T9pGD7NKG1uz0naz5+zw\nbC2IOZKeveVFjs2L9VG8Z2TFdLdcGpjSdQIO5qUPa4uonpH6jyE8RKbVUJbr89TJvpmFXheCp4qY\nKJR6duh9FuXYtCQ9xQGxZbzA8y39KYBrfBlkz/olp9y432Rqn6U6TGg5xWV3qbbKspe/dl9N/YHH\nbYyKqgXi/kXpVc7ieKGjrI6t6+WJrD6a7Y2c1uv4lzpyUSs6SQA8Gz/rfOG/AuNq6jYRgueelrcX\noAPDTdPHNURgRmlzrCRWZKkmCVReKjXVjuWxjoFr32Uyuz/OVyWS6duG1qLIlJqSlOjZH1bkuFYg\n0idVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/B\n",
      "QnJvYWR3YXlQLnR0Zg==.png\n",
      "notMNIST_small/B/QnJvYWR3YXlQLnR0Zg==.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABj0lEQVR4nHWSPWhUQRSFv7l3lkRS\nmICgIv4QYU0jgojiH8YiVQTTaBmENGKhkk5BLFIIdlqJiEHEQlAjIltZWCiClUJAizSCQsAigmtY\n2ffmWJhn5i3uqc7lmztn5s7QUqlKpaTO8of7U8MEB2JCrEl2493Ihi3Nw9Or8ze/eRIvVFSNScdw\ngNG5n6szGDzP4QkG3D0Ehh9qDmNhHUoHMQCLxjVdgGc53P8Xgjnzv0ctkamsTBJX0qU6LP655Mut\n01bmsFa82dkXBn7RJxPEjnatUwJCVY2/jbVtBeBJhNg9cnSyF8pDgaHuxscPWvXMgI5/nmqk5BNL\nizO1VwHgY3vh+5furq2Xb4UQff0EELCVQ6fGNy3uOzNISNzNZ7sbCwDGec3i3MthEyPEGGOD62k7\nZnmgASqKoii53T7bA2Nlkq0sHeiBnvuiH7S0eew1li+msXYjdy52nhIzqDBIoxQkdc9dnf7hPPnP\n17SxR5rFap0wOTI0MLRtz97mp4lXXsLLbAhJkjpf39856TjwB+ap8SfcsghMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/C\n",
      "RHJvcGxldC5vdGY=.png\n",
      "notMNIST_small/C/RHJvcGxldC5vdGY=.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABuklEQVR4nF2SvWtUURDFz8x9u4is\n4BaBQIhEN41gIQgaGwuLCCKIpNDCwkYFi/0HrCwsBEljsZWthQhibGy2iSwqLviBYGEMCiL4gRIi\nybp7Z47FvdckO817zO/NnPvOPQIACPRwaP5oa7KBtU/9+y9ciFwSUL/49C9L+dIMtMCAuT6dFs1J\nt2hc2VdowJUhY+RWDbiIkFl7JyKNXUhip2n2X87M3Qe8nSebH1hYTC/O9cNZ8zpHZYr+4+Pn77+/\nPZlLW7FnhZ6Yj+6dnNBdEzN7IYnhfGFcX8jfQ0P+yQ5jPuEl1IJARKUYgOfpOJG97d28YX/Z9YDK\ncdhMUPASjnGYtQUb4wjQP+lJNDEuCf2CrNTa0ZdQBeibJEWc2D6psGjQXuopTk3FcsNSef3C4llg\n+meyKPIOKgUgWgUcfEbnOaCTjXe2IRIEguraGuMmbwKzv7K7xrstAeoHrr6mGY0LCLjMUaEbvcfd\ntwPSnEN2AShulVvOYYmRPuTqLABR3KAn7HEUjfQY2Z9OYQg4s0qnRS8xIjuNHBRRNNqvfCt9m4+O\nQxXZleCsHZs/0prcXePX98sP30FJ4B8VDF9AcrR7ywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/D\n",
      "RmFpcmZpZWxkTEgtSGVhdnlTQy5vdGY=.png\n",
      "notMNIST_small/D/RmFpcmZpZWxkTEgtSGVhdnlTQy5vdGY=.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABeElEQVR4nG3TPWtUYRCG4XvmPZs1\nG0EbC1OZKJpSELVQwUaCH6ksBRWC/gHBpLdKrWhpo4JIKrFR0EqwEEGwCC4mCCuI+LGoLJiz7zwW\nOUqy50zzFBcz08zYj51sqvJz792zF19JGcDmxs9ckoHs28vhrkMd4OOj22uGADiqLKnUdWDqjsos\n/bwGBoyl4xXOp7bDgoZRSvfbZpA4XOFVCrzgoYaKUsvutgmvUIDb9CBCWtcNkrO1wlafWECRFw/m\nUcTtAQZGsUgNQ69/ucB1dm8drdclwLT9XA1xrQEgnawjrCLA7EAdje8bE9jd1DmocqIJy//rG7BT\n5e8mnAAg+FRHMbmRWmnCKQwwe15Di84+DGT9x3Vk/x45hC33GnDWA8IGSzaKptZlDGVf6PootuLC\nTHhE6+atlBlLx/7dUGqngpkvuSylJdwAjlR4EeDEB2Wpex43KE5tOw2AM/9nfXJ2DvzNvbv9lAHr\n79iy8/3K26evhtU7/AUWL7Ov/qvsPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/E\n",
      "Q29yaW50aGlhbi50dGY=.png\n",
      "notMNIST_small/E/Q29yaW50aGlhbi50dGY=.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB3klEQVR4nG2SvWsUURTFz7t3NiEi\n7iJJYHGRBVkLMRqEFEIUohYWWihWKjZpF9Q/QRAsxNYilR8QbLUREo0gFgYhRlELFZfshwkBIe5O\nYjLz3jsWk50diac9j3vu+d1n0JXSIz92+kBheGB15o5FRkaA8UdLliQ9o3FIzxPBxJxjIs9v+2AA\nmGSk23PrunGL8+sm1+fbP563DNM4HPxArlw2vZTeKhitk62jEE0U9DzB4WWyOYoAO2Swd5ExzyG3\n04PBNDd5H/ofT3GFln+OoU9VRYwxWXeoRstGKdM69QNUy05Zuvewo7oRhmG45dOKGP7OLhm6zc6v\nldrC67PdMZO0JBlZ0lnnSFpObbeS5A1zbp2iIgC09gA+Ce9/ddwpTf3qUnmw0D+Qc78bb8KU7ElL\n73gh2zNto7jLiPUSVDUIgiAXaJZ/sU2uHcpetyshVn/C5i9B/0GTknhGy/g8NJOqAggAxU1GnhtV\ngagAgCiAQj4J3fWWsXN8ccIAEDGAnHrSbNxIvlH5E21sGc1NVhTA/uo7RzpeAwDB0GPSb8Vk+GXm\n6fwaGUWeHyvJygZnZj2ZYCY9PVu3B7dhGONl5OLEkXyya+fz+9mXbZOSUnpIcaRS3O2Wvy40PaCe\nfwGgrfXl8pgcwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/F\n",
      "QmFza2VydmlsbGVCUS1JdGFsaWMub3Rm.png\n",
      "notMNIST_small/F/QmFza2VydmlsbGVCUS1JdGFsaWMub3Rm.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABr0lEQVR4nG2Rv2tUQRSFvzsz2V1I\nSAiGREkQDIoQ0MIqwYAWCUoESwV7wVa08j+wEy3SpVWxFJZVsBDDQpSgjYoWKUQQRYkEf+2+N3Ms\n3tvNW82t5s7hO9x7LgA4DurfytQOAAhds/ELR6L/fDO65BuNQ+fHGKjr+q21XjP3Ia278m1+yJ3E\n0fR1730I9Tc3LPUxY+Kr9G0SK1o3vL3RI/Es7uvS/uIEYLKfL6ZCTxTnEE1cKrpkV6d3XRtbSn8O\n4wCjNjCpZ1G52piBcW8Vh/P9aTlLRgsvvO2/+BZHin1bv6lcC6EegnErzuB2XR1znahNAIauqF0s\nVE7r0nIts6n7TrKjx2nh8yr5SFHdIvI8zuOr2oEd6dP08MjIxMK63tXLnAAIdkkd3S2aea2WYHky\nrQBNCzle0KqCxuhHpZ0ZHAQud8aromdZmZ7gAM/G896SoSBXyEMReuT2+wqHEV4r5seq81dcT8Rc\nr5wVRD/v8kJnXMbj5AVATFXS8UyZTu/har5ms7+itkZD+E8EWFNXL/f4t9lTk0tLMtKDpz8efjcN\nqHckpeIa22PVZIC/HyK55OW+lUIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/G\n",
      "R2Fkem9veEJvbGQub3Rm.png\n",
      "notMNIST_small/G/R2Fkem9veEJvbGQub3Rm.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACCElEQVR4nFWSS0uWURSFn3PO6yeC\nF7IiQygJQURCGgRRCYpQk5CUJiV2QwcRFQ2in9EkiKJBVyc1tAKli4E2scAUE8NJEISEXaRCv3P2\navC+Jo3X3ms/e+3twPlUd3A7sDz1iZBO3c6mO1acAHCe/kWZJP0Yu9JO1Tupj0Aucl1mZlaOktLb\nwZvSMB4AzzWVk6Rk+v31p0kxaqkBB0CvoklKmj3Z0tjSeem12aou5L6lKSVJSSM1RfXhOWlxCw44\nrrxvoZ4K73zIPFvHpX4CMKwoKeoqFTkgGY13e3KgfAvTPjwFY/DrZetc3wEsgEsJunnuBJIkmVoJ\nVO7wgG8aHNVcJY6scKDhg09tLyffq6GlrYbYeuyhT5gkKWmIEp2KkqQYoyYA/wsA0YyY/xgMSwoh\naP8RQrZcLQeiCbkvXV27mo5uEmDh/IgxVgQ0VYFzjozHKkuypA78NAIcu3fi5EKAfDdjCD+aAuCs\ntAeHpQQI8e0Rve3+1QwGiENFGg4wxgdmq/vgYj5Dn7fhIPBM0RQP0H26FmoXCqQzBBxVM0pl3clj\nDpwo7jLhvAvslZKWm/E+OPDc15qkpLNkGfcUo85tPFjtpNYk00oPDMjWdIv1o+HZ/EIxyRRHh/9o\nVU9LBXiulm7IUjKTYllv6jca87/unZdJyaQHNf9p4AJ1l2dNaelJD+6f518x3UvGy1QPGgAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/H\n",
      "Q2Fzc2FuZHJhRUYtQm9sZC5vdGY=.png\n",
      "notMNIST_small/H/Q2Fzc2FuZHJhRUYtQm9sZC5vdGY=.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAs0lEQVR4nOWRvZGCMQxEn9YKyRka\noQjKIaAIAnoCEoZaqICxpQuOP3++OchRJj+ttLO2A+Hrs4LXUix3VZCZuaJ0jMIqM9ODVipD1WgF\nFykboSmFxveX018CHcDM+pBuvQNcc5Ju5fqA80VpHSxtDmAJ1P43AeR3+K+hGCdMHyhDm/NgaLkN\n4ZCcjoMoyNvNWRmUs6ehRg9pAe+yDeIvxxlhuNDv7olIAt8TfmGiTS7HKn4A5OpEyhmtgCMAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/I\n",
      "SG9iYml0IFNGLnR0Zg==.png\n",
      "notMNIST_small/I/SG9iYml0IFNGLnR0Zg==.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCUlEQVR4nH3SP0tCcRTG8e/veAu6\nZnLTjCBwEIIGIaHNRV+Ab6Ehghp6A72AXkh7NLYFRotjiwQODZFBRTfEK7fQe0+Dm/fYM54P589w\nAFc4vn+LprqQZHjhgZ6e1cjGeSG4/dskSRf7VMMrgMuXrKjOujvA5oNl+ngEQqtsLCTt3zg8WiUL\nB70IhMOihf2eA/Erqxa+DhSktmJZ/DUCpJyz8DMEkDVnYTQBEN/E+AdAcibOEgD5VQvnkUlqlTfW\nASQ2Owt5ALEHBsV/MB/4y5GgAsjUvnZ3zy09iEZbQd4TE7cadYd8jG09OFFEn8YmbjfrTuh+21ur\n5wqlu8yzz/NcBZrXIxOHnT+wjL8+/JRfjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/J\n",
      "RWx3b29kLnR0Zg==.png\n",
      "notMNIST_small/J/RWx3b29kLnR0Zg==.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACDklEQVR4nF2SS0iUYRSGn/Odb0wZ\nb5BgeAmFWrRIK0ijLAhXESR0I9KIXJS1CmoVkV0EKQmCVlkEXYgoiKA2tZgyEoyCDEoDcVGO5iUs\nNCNn/vlPiynw92wfOO973vPCwlHB1V3oSQ6+ttu4CEIo6/o8/KBtbTEn/lRGqWP/9LOmAgSg/xg+\nspSrVolT78Rz7ika2Rrj1rcqHODYnCxcJFmdsL44AkLedFME5Z7/bfPWhQLK85sRxa0WpMxmVuDA\n0z4QPaU9bZm0ncWDY0NmVVSz9q2lbCCGIBRNtkWgJ7/P0rYNBaH/+sIULPC/mic8u7KJTCyPBhj4\n4ZPQWJgRYDq+KN1A7iaoWo0DUuoA570Tcd4Djm6oASBvziEaBkFoFgYBEJKYoBzBqBz3kKnYu6mi\nIJwZeXzfMJkaKk1hEi4pf+RVO46OTCaT45MH7r0Yd6HYPB9ArKIkAbV2HAWlwerwSnxqrAhRDg95\ntj+xZnLUxzT2vr8UR6ftRnG8vAxjZjdQQCi5M9KbGE0dQlHWza6ETrPREgQQoezgmdZlOMSTuIaS\n/9HsNDkALtsLh3o6vhQj0BiG6R3ENIu9V69w0TZmXbakbbYFEfXee+8Q1r+zPdl6KQ2fzB7WZBsp\nFO18ZaNb/nlEM7n7jtQz2Dv4I720ur4m/vVK95xm/rdHkDWner6bmf18c6khhmSf9ReK9bzt3YBI\n7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "def display_image(data_folders, sample_size):\n",
    "    for folder in data_folders:\n",
    "        print(folder)\n",
    "        image_files = os.listdir(folder)\n",
    "        image_sample = random.sample(image_files, sample_size)\n",
    "        for image in image_sample:\n",
    "            print(image)\n",
    "            image_file = os.path.join(folder, image)\n",
    "            print(image_file)\n",
    "            i = Image(filename=image_file)\n",
    "            display(i)\n",
    "\n",
    "display_image(test_folders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "pixel_depth = 255.0\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                         pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exeption('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "            \n",
    "        except IOError as e:\n",
    "            print('Could not read : ', image_file, ':', e, '- it\\'s ok, skipping')\n",
    "    print (dataset.shape)\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('May fewer images than expected : %d < %d' % (num_images, min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor: ', dataset.shape)\n",
    "    print ('Mean: ', np.mean(dataset))\n",
    "    print('Standard deviation:' , np.std(dataset))\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_name = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_name.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to' , set_filename, ':' , e)\n",
    "    return dataset_name\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2 Fullyconnected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf                  \n",
    "from six.moves import cPickle as pickle   \n",
    "from six.moves import range              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "[3 6 4 ..., 6 9 0]\n",
      "[[[-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  ..., \n",
      "  [ 0.42156863  0.5         0.42156863 ..., -0.4254902  -0.5        -0.49607843]\n",
      "  [ 0.05686275  0.5         0.18627451 ..., -0.5        -0.49215686 -0.5       ]\n",
      "  [-0.42156863 -0.20588236 -0.31568629 ..., -0.49215686 -0.5        -0.5       ]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.49607843 -0.5        -0.5       ]\n",
      "  ..., \n",
      "  [-0.5        -0.5        -0.5        ..., -0.17450981  0.5         0.47254902]\n",
      "  [-0.5        -0.5        -0.5        ...,  0.46470588  0.5         0.19019608]\n",
      "  [-0.5        -0.5        -0.5        ...,  0.46078432  0.13529412\n",
      "   -0.42156863]]\n",
      "\n",
      " [[ 0.22941177  0.5         0.49215686 ...,  0.5         0.5         0.5       ]\n",
      "  [ 0.2254902   0.49607843  0.48823529 ...,  0.5         0.5         0.5       ]\n",
      "  [ 0.24509804  0.5         0.5        ...,  0.5         0.5         0.5       ]\n",
      "  ..., \n",
      "  [ 0.24509804  0.5         0.5        ...,  0.5         0.5         0.5       ]\n",
      "  [ 0.2254902   0.49607843  0.48823529 ...,  0.5         0.5         0.5       ]\n",
      "  [ 0.22941177  0.5         0.49215686 ...,  0.5         0.5         0.5       ]]\n",
      "\n",
      " ..., \n",
      " [[-0.5        -0.5        -0.5        ..., -0.5        -0.48823529 -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.48823529 -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.48823529 -0.5       ]\n",
      "  ..., \n",
      "  [-0.5        -0.5        -0.49215686 ..., -0.5        -0.48823529 -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.48823529 -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.49607843 -0.5       ]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ...,  0.49607843  0.5         0.30000001]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.01764706 -0.16666667\n",
      "   -0.36666667]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  ..., \n",
      "  [ 0.39411765  0.5         0.48823529 ..., -0.5        -0.5        -0.5       ]\n",
      "  [ 0.25294119  0.5         0.49607843 ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.41764706  0.04509804  0.35882354 ..., -0.5        -0.5        -0.5       ]]\n",
      "\n",
      " [[-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  [-0.5        -0.5        -0.5        ..., -0.5        -0.5        -0.5       ]\n",
      "  ..., \n",
      "  [-0.48039216 -0.5        -0.02941176 ..., -0.04509804 -0.5        -0.48039216]\n",
      "  [-0.5        -0.26078433  0.46078432 ...,  0.47254902 -0.21764706 -0.5       ]\n",
      "  [-0.26862746  0.40980393  0.5        ...,  0.5         0.44509804\n",
      "   -0.15490197]]]\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'    # pickle file name\n",
    "\n",
    "with open(pickle_file, 'rb') as f:    # pickle file 이진 읽기 모드로 열기(r : read, b : binary) \n",
    "    save = pickle.load(f)    # load pickle file\n",
    "    train_dataset = save['train_dataset']    # train_dataset 으로 저장한 data 가져 오기\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save    # memory 해제\n",
    "    \n",
    "    \n",
    "    print('Training set', train_dataset.shape, train_labels.shape)    # dataset 의 형태 확인\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    print(test_labels)\n",
    "    print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[[4]\n",
      " [9]\n",
      " [6]\n",
      " ..., \n",
      " [2]\n",
      " [4]\n",
      " [4]]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[[1]\n",
      " [9]\n",
      " [3]\n",
      " ..., \n",
      " [8]\n",
      " [9]\n",
      " [8]]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[[3]\n",
      " [6]\n",
      " [4]\n",
      " ..., \n",
      " [6]\n",
      " [9]\n",
      " [0]]\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)    # 3차원 행렬을 float32 타입의 2차원 행렬로 변경\n",
    "    print(np.arange(num_labels))\n",
    "    print(labels[:,None])\n",
    "\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)    # array slice 를 통해 lavels 를 2\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)    # dataset 차수 변경하기\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set' , train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 6  7  8]\n",
      "  [ 9  0  1]]\n",
      "\n",
      " [[16 17 18]\n",
      "  [19 10 11]]]\n",
      "(3, 2, 3)\n",
      "[[  1.   2.   3.   4.   5.   6.]\n",
      " [  6.   7.   8.   9.   0.   1.]\n",
      " [ 16.  17.  18.  19.  10.  11.]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[[1, 2, 3], [4, 5, 6]],[[6, 7, 8], [9, 0, 1]],[[16, 17, 18], [19, 10, 11]]])\n",
    "print(arr)\n",
    "print(arr.shape)\n",
    "arr = arr.reshape((-1, 2*3)).astype(np.float32)\n",
    "print(arr)\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multinormial classification ( softmax regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'Const:0' shape=(10000, 784) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])    # train_dataset 을 slice 하여 행을 0~9999(10000개)사용 열은 유지(10)\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    print(tf_train_dataset.get_shape)\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))    # 784 X 10 weight를 랜덤(정규화?)하게 초기화\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))    # 10 X 1 biases 를 0 로 초기화  \n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases    # hyperthsis = wx + b\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))    # tensor 에서 제공하는 softmax , closs entropy 한방에\n",
    "                                                                         # cast func = -log(y^)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)    # loss 를 gradientdesent 로 optimizer 하여 최소 값 구함. learnning rate 는 0.5 사용\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)    # train data set 의 예측 값\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)    # train 과 동일하게 validation 예측\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 20.898104\n",
      "Training accuracy: 7.6%\n",
      "Loss at step 100: 2.327554\n",
      "Training accuracy: 70.6%\n",
      "Loss at step 200: 1.866469\n",
      "Training accuracy: 73.4%\n",
      "Loss at step 300: 1.631458\n",
      "Training accuracy: 74.2%\n",
      "Loss at step 400: 1.474042\n",
      "Training accuracy: 74.7%\n",
      "Loss at step 500: 1.355821\n",
      "Training accuracy: 75.0%\n",
      "Loss at step 600: 1.261784\n",
      "Training accuracy: 75.2%\n",
      "Loss at step 700: 1.184033\n",
      "Training accuracy: 75.4%\n",
      "Loss at step 800: 1.118059\n",
      "Training accuracy: 75.5%\n",
      "Test accuracy: 82.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):    # accuracy 를 % 로 표현\n",
    "    #print(np.argmax(predictions, 1))\n",
    "    #print(np.argmax(labels, 1))\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "           / predictions.shape[0])    # 10000 개의 train data set 중 predict 값이 맞은 개수의 합을 10000으로 나누고 100으로 곱합\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):    # 801번 train\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if(step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))    # step 과 loss 출력\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))    # eval()? test dataset 정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                     shape = (batch_size, image_size * image_size))    # 128 x 784\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)    # valiable initialize\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.219501\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 14.7%\n",
      "Minibatch loss at step 500: 1.435687\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1000: 1.503235\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1500: 0.887524\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 2000: 1.067979\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 2500: 0.724542\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 3000: 1.100917\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.8%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)    # 0, 128, 256, ... , \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)    # feed_dict=> placeholder 에 값 설정\n",
    "        if(step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem. 1-hidden layer(activate function =  ReLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_node = 1024\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                     shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_node]))\n",
    "    b1 = tf.Variable(tf.zeros([num_node]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([num_node, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)    # hidden layer\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w2) + b2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 305.156738\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 36.1%\n",
      "Minibatch loss at step 500: 19.512383\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1000: 22.571728\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 6.235578\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2000: 6.087286\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2500: 3.662671\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 3.890370\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if(step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_node = 1024\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                     shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_node]))\n",
    "    b1 = tf.Variable(tf.zeros([num_node]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([num_node, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1) \n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w2) + b2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 362.009460\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 24.6%\n",
      "Minibatch loss at step 500: 37.268814\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 29.013060\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1500: 16.397827\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 2000: 8.507425\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2500: 2.542193\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 3000: 3.850550\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 89.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if(step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 3 Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    \n",
    "    del save\n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set' , valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "           / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.547451\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 15.3%\n",
      "Minibatch loss at step 500: 2.663783\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1000: 2.080396\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 1500: 1.023589\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2000: 0.936654\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500: 0.649134\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.854148\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96e3b06a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assignment 4 convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-6f4376fd9e3c>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-6f4376fd9e3c>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    with graph.as_default()\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placehodler(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
